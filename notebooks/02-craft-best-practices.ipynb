{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 · CRAFT Trainer Best Practices\n",
        "\n",
        "This notebook expands on the basic workflow with:\n",
        "\n",
        "1. Chat-template tokenisation with assistant-only masks.\n",
        "2. Self-alignment (no external positives) while reusing assistant masks.\n",
        "3. Parameter-efficient finetuning via QLoRA.\n",
        "4. Custom `craft_beta` ratios and mixed precision hints.\n",
        "\n",
        "> ⚠️ Adapt dataset subsampling, precision flags, and LoRA ranks for your hardware.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Optional environment setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -U \"contrastive-ft[trl,peft] @ git+https://github.com/omarkamali/craft\"\n",
        "# !pip install -U \"datasets>=2.19\" \"transformers>=4.43\" \"trl>=0.9\" \"accelerate>=0.30\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "from craft.config import CRAFTSFTConfig\n",
        "from craft.data import CRAFTCollator, make_craft_datasets\n",
        "from craft.trainers import CRAFTSFTTrainer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Tokeniser helpers with chat templates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_LENGTH = 1024\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def encode_chat(example):\n",
        "    encoded = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=False,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors=\"pt\",\n",
        "        return_dict=True,\n",
        "        return_assistant_tokens_mask=True,\n",
        "    )\n",
        "    input_ids = encoded[\"input_ids\"][0]\n",
        "    attention_mask = encoded[\"attention_mask\"][0]\n",
        "    assistant_mask = encoded[\"assistant_masks\"][0]\n",
        "    labels = input_ids.clone()\n",
        "    labels = labels.masked_fill(assistant_mask == 0, -100)\n",
        "    return {\n",
        "        \"input_ids\": input_ids.tolist(),\n",
        "        \"attention_mask\": attention_mask.tolist(),\n",
        "        \"labels\": labels.tolist(),\n",
        "        \"assistant_mask\": assistant_mask.tolist(),\n",
        "        \"attention_mask_tgt\": assistant_mask.tolist(),\n",
        "    }\n",
        "\n",
        "def encode_contrastive(example):\n",
        "    anchor = tokenizer(\n",
        "        example[\"premise\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\"\n",
        "    )\n",
        "    positive = tokenizer(\n",
        "        example[\"hypothesis\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\"\n",
        "    )\n",
        "    return {\n",
        "        \"input_ids\": anchor[\"input_ids\"][0].tolist(),\n",
        "        \"attention_mask\": anchor[\"attention_mask\"][0].tolist(),\n",
        "        \"input_ids_tgt\": positive[\"input_ids\"][0].tolist(),\n",
        "        \"attention_mask_tgt\": positive[\"attention_mask\"][0].tolist(),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and preprocess datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sft_raw = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train[:0.3%]\")\n",
        "contrastive_raw = load_dataset(\"sentence-transformers/all-nli\", split=\"train[:0.3%]\")\n",
        "\n",
        "tokenized_sft = sft_raw.map(encode_chat, remove_columns=sft_raw.column_names)\n",
        "tokenized_contrastive = contrastive_raw.map(\n",
        "    encode_contrastive, remove_columns=contrastive_raw.column_names\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Bundle + collator (self-align friendly)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bundle = make_craft_datasets(tokenized_sft, strategy=\"self_align\")\n",
        "collator = CRAFTCollator()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Prepare LoRA-wrapped model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", load_in_4bit=True)\n",
        "lora_cfg = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05, target_modules=[\"q_proj\", \"v_proj\"])\n",
        "model = get_peft_model(base_model, lora_cfg)\n",
        "model.config.use_cache = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Configure trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = CRAFTSFTConfig(\n",
        "    output_dir=\"./outputs/craft-best-practices\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=1e-4,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=5,\n",
        "    save_steps=50,\n",
        "    bf16=True,\n",
        "    craft_alpha=0.7,\n",
        "    craft_beta=0.4,\n",
        "    craft_pooling=\"cls\",\n",
        "    craft_assistant_mask_strategy=\"auto\",\n",
        "    craft_length_strategy=\"oversample\",\n",
        ")\n",
        "\n",
        "trainer = CRAFTSFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_sft,\n",
        "    data_collator=collator,\n",
        "    craft_bundle=bundle,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Inspect metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.state.log_history[-5:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save adapters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save_pretrained(\"./outputs/craft-best-practices\")\n",
        "tokenizer.save_pretrained(\"./outputs/craft-best-practices\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
