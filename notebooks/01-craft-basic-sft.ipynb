{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 Â· Basic CRAFT SFT Trainer\n",
        "\n",
        "This notebook demonstrates a minimal fine-tuning pipeline using `CRAFTSFTTrainer`.\n",
        "It now relies on `tokenizer.apply_chat_template` to format conversations and\n",
        "derives assistant-only loss masks via `return_assistant_tokens_mask`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Optional setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -U \"contrastive-ft @ git+https://github.com/omarkamali/craft\"\n",
        "# !pip install -U \"datasets>=2.19\" \"transformers>=4.43\" \"trl>=0.9\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "from craft.config import CRAFTSFTConfig\n",
        "from craft.data import CRAFTCollator, make_craft_datasets\n",
        "from craft.trainers import CRAFTSFTTrainer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load tiny demo splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sft_dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train[:0.1%]\")\n",
        "contrastive_dataset = load_dataset(\"sentence-transformers/all-nli\", split=\"train[:0.1%]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Tokenizer & chat templating helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "MAX_LENGTH = 512\n",
        "\n",
        "\n",
        "def apply_chat(example):\n",
        "    encoded = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=False,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors=\"pt\",\n",
        "        return_dict=True,\n",
        "        return_assistant_tokens_mask=True,\n",
        "    )\n",
        "    input_ids = encoded[\"input_ids\"][0]\n",
        "    attention_mask = encoded[\"attention_mask\"][0]\n",
        "    assistant_mask = encoded[\"assistant_masks\"][0]\n",
        "    labels = input_ids.clone()\n",
        "    labels = labels.masked_fill(assistant_mask == 0, -100)\n",
        "    return {\n",
        "        \"input_ids\": input_ids.tolist(),\n",
        "        \"attention_mask\": attention_mask.tolist(),\n",
        "        \"labels\": labels.tolist(),\n",
        "        \"assistant_mask\": assistant_mask.tolist(),\n",
        "    }\n",
        "\n",
        "\n",
        "def tokenize_contrastive(example):\n",
        "    anchor = tokenizer(\n",
        "        example[\"premise\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\"\n",
        "    )\n",
        "    positive = tokenizer(\n",
        "        example[\"hypothesis\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\"\n",
        "    )\n",
        "    return {\n",
        "        \"input_ids\": anchor[\"input_ids\"][0].tolist(),\n",
        "        \"attention_mask\": anchor[\"attention_mask\"][0].tolist(),\n",
        "        \"input_ids_tgt\": positive[\"input_ids\"][0].tolist(),\n",
        "        \"attention_mask_tgt\": positive[\"attention_mask\"][0].tolist(),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tokenise datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_sft = sft_dataset.map(apply_chat, remove_columns=sft_dataset.column_names)\n",
        "tokenized_contrastive = contrastive_dataset.map(\n",
        "    tokenize_contrastive, remove_columns=contrastive_dataset.column_names\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build dataset bundle & collator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bundle = make_craft_datasets(\n",
        "    tokenized_sft, contrastive_dataset=tokenized_contrastive, strategy=\"paired_dataset\"\n",
        ")\n",
        "collator = CRAFTCollator()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load base model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n",
        "model.config.use_cache = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Trainer configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = CRAFTSFTConfig(\n",
        "    output_dir=\"./outputs/craft-basic\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    craft_alpha=0.5,\n",
        "    craft_beta=0.6,\n",
        ")\n",
        "\n",
        "trainer = CRAFTSFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_sft,\n",
        "    data_collator=collator,\n",
        "    craft_bundle=bundle,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.save_model()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
