{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03a · Using `InfoNCELoss` with `transformers.Trainer`\n",
        "\n",
        "This notebook shows how to integrate the core `InfoNCELoss` module with the standard Hugging Face `Trainer`.\n",
        "\n",
        "We demonstrate:\n",
        "\n",
        "1. Preparing SFT and contrastive datasets with `tokenizer.apply_chat_template` so assistant tokens are masked.\n",
        "2. Extending `Trainer` to combine SFT and InfoNCE losses via `combine_craft_losses`.\n",
        "3. Logging basic metrics such as contrastive accuracy.\n",
        "\n",
        "> ⚠️ Update checkpoints, dataset sizes, and output directories before running.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Optional environment setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -U \"contrastive-ft @ git+https://github.com/omarkamali/craft\"\n",
        "# !pip install -U \"datasets>=2.19\" \"transformers>=4.43\" \"accelerate>=0.30\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "from craft.data import CRAFTCollator, make_craft_datasets, CRAFTMixedDataLoader\n",
        "from craft.losses import InfoNCELoss, combine_craft_losses\n",
        "from craft.metrics import compute_contrastive_accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load demo datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sft_raw = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train[:0.2%]\")\n",
        "contrastive_raw = load_dataset(\"sentence-transformers/all-nli\", split=\"train[:0.2%]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Tokeniser helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "MAX_LENGTH = 1024\n",
        "\n",
        "def encode_sft(example):\n",
        "    encoded = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=False,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors=\"pt\",\n",
        "        return_dict=True,\n",
        "        return_assistant_tokens_mask=True,\n",
        "    )\n",
        "    input_ids = encoded[\"input_ids\"][0]\n",
        "    attention_mask = encoded[\"attention_mask\"][0]\n",
        "    assistant_mask = encoded[\"assistant_masks\"][0]\n",
        "    labels = input_ids.clone().masked_fill(assistant_mask == 0, -100)\n",
        "    return {\n",
        "        \"input_ids\": input_ids.tolist(),\n",
        "        \"attention_mask\": attention_mask.tolist(),\n",
        "        \"labels\": labels.tolist(),\n",
        "        \"assistant_mask\": assistant_mask.tolist(),\n",
        "    }\n",
        "\n",
        "def encode_contrastive(example):\n",
        "    anchor = tokenizer(\n",
        "        example[\"premise\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\"\n",
        "    )\n",
        "    positive = tokenizer(\n",
        "        example[\"hypothesis\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\"\n",
        "    )\n",
        "    return {\n",
        "        \"input_ids\": anchor[\"input_ids\"][0].tolist(),\n",
        "        \"attention_mask\": anchor[\"attention_mask\"][0].tolist(),\n",
        "        \"input_ids_tgt\": positive[\"input_ids\"][0].tolist(),\n",
        "        \"attention_mask_tgt\": positive[\"attention_mask\"][0].tolist(),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tokenise datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_sft = sft_raw.map(encode_sft, remove_columns=sft_raw.column_names)\n",
        "tokenized_contrastive = contrastive_raw.map(\n",
        "    encode_contrastive, remove_columns=contrastive_raw.column_names\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build bundle and collator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bundle = make_craft_datasets(\n",
        "    tokenized_sft, contrastive_dataset=tokenized_contrastive, strategy=\"paired_dataset\"\n",
        ")\n",
        "collator = CRAFTCollator()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model and loss objects\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n",
        "model.config.use_cache = False\n",
        "\n",
        "craft_loss = InfoNCELoss(\n",
        "    temperature=0.08,\n",
        "    pooling=\"last_token\",\n",
        "    hidden_size=model.config.hidden_size,\n",
        ")\n",
        "CRAFT_ALPHA = 0.6\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Custom Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CraftTrainer(Trainer):\n",
        "    def __init__(self, *args, craft_bundle, craft_loss, craft_alpha=0.5, length_strategy=\"oversample\", **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.craft_bundle = craft_bundle\n",
        "        self.craft_loss = craft_loss.to(self.model.device)\n",
        "        self.craft_alpha = craft_alpha\n",
        "        self.craft_length_strategy = length_strategy\n",
        "\n",
        "    def get_train_dataloader(self):\n",
        "        base_loader = super().get_train_dataloader()\n",
        "        contrastive_loader = self._build_contrastive_loader(base_loader.batch_size)\n",
        "        return CRAFTMixedDataLoader(\n",
        "            base_loader,\n",
        "            contrastive_loader,\n",
        "            beta=0.5,\n",
        "            gradient_accumulation_steps=self.args.gradient_accumulation_steps,\n",
        "            length_strategy=self.craft_length_strategy,\n",
        "        )\n",
        "\n",
        "    def _build_contrastive_loader(self, batch_size):\n",
        "        from torch.utils.data import DataLoader\n",
        "\n",
        "        return DataLoader(\n",
        "            self.craft_bundle.contrastive_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=self.data_collator,\n",
        "        )\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        batch_type = inputs.pop(\"craft_batch_type\", \"sft\")\n",
        "        if batch_type == \"craft\":\n",
        "            loss = self._compute_craft_loss(model, inputs)\n",
        "            outputs = None\n",
        "        else:\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs.loss\n",
        "            loss = combine_craft_losses(sft_loss=loss, contrastive_loss=None, alpha=self.craft_alpha).total_loss\n",
        "            self.log({\n",
        "                \"loss/craft_sft\": float(loss.detach()),\n",
        "                \"loss/craft_total\": float(loss.detach()),\n",
        "            })\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def _compute_craft_loss(self, model, inputs):\n",
        "        anchor_ids = inputs[\"input_ids\"]\n",
        "        anchor_mask = inputs[\"attention_mask\"]\n",
        "        positive_ids = inputs[\"input_ids_tgt\"]\n",
        "        positive_mask = inputs[\"attention_mask_tgt\"]\n",
        "\n",
        "        anchor_outputs = model(input_ids=anchor_ids, attention_mask=anchor_mask, output_hidden_states=True)\n",
        "        positive_outputs = model(input_ids=positive_ids, attention_mask=positive_mask, output_hidden_states=True)\n",
        "\n",
        "        loss, details = self.craft_loss(\n",
        "            anchor_outputs.hidden_states[-1],\n",
        "            positive_outputs.hidden_states[-1],\n",
        "            anchor_mask,\n",
        "            positive_mask,\n",
        "            return_details=True,\n",
        "        )\n",
        "        total = combine_craft_losses(sft_loss=None, contrastive_loss=loss, alpha=self.craft_alpha)\n",
        "        accuracy = compute_contrastive_accuracy(details[\"anchor_embeddings\"], details[\"positive_embeddings\"])\n",
        "        self.log({\n",
        "            \"loss/craft_contrast\": float(loss.detach()),\n",
        "            \"loss/craft_total\": float(total.total_loss.detach()),\n",
        "            \"metrics/craft_contrastive_accuracy\": float(accuracy.detach()),\n",
        "        })\n",
        "        return total.total_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training arguments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./outputs/craft-transformers-trainer\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-5,\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    num_train_epochs=1,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Instantiate and train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = CraftTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_sft,\n",
        "    data_collator=collator,\n",
        "    craft_bundle=bundle,\n",
        "    craft_loss=craft_loss,\n",
        "    craft_alpha=CRAFT_ALPHA,\n",
        "    length_strategy=\"oversample\",\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Inspect logs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.state.log_history[-5:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Save artefacts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.save_model(\"./outputs/craft-transformers-trainer\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
